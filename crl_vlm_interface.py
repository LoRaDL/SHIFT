"""
CRL VLMäº¤äº’æ¥å£ - LMStudioæœ¬åœ°ç‰ˆ
"""
import json
import os
import requests
from PIL import Image
import base64
from io import BytesIO
from typing import List, Dict, Any, Optional

class LMStudioVLMInterface:
    """LMStudioæœ¬åœ°VLMæ¥å£"""
    
    def __init__(self, base_url: str = "http://localhost:1234"):
        self.base_url = base_url
        self.context_data = None
    
    def load_context(self, context_file: str):
        """åŠ è½½VLMä¸Šä¸‹æ–‡æ•°æ®"""
        with open(context_file, 'r', encoding='utf-8') as f:
            self.context_data = json.load(f)
        print(f"Loaded context with {len(self.context_data['keyframes'])} keyframes")
    
    def prepare_images_for_vlm(self, max_size: tuple = (512, 512), add_labels: bool = True) -> List[str]:
        """
        å‡†å¤‡å›¾åƒç”¨äºVLMè¾“å…¥
        
        Args:
            max_size: æœ€å¤§å›¾åƒå°ºå¯¸ (width, height)
            add_labels: æ˜¯å¦åœ¨å›¾ç‰‡ä¸Šæ·»åŠ æ•°å­—æ ‡ç­¾
            
        Returns:
            List[str]: Base64ç¼–ç çš„å›¾åƒåˆ—è¡¨
        """
        if not self.context_data:
            raise ValueError("No context data loaded. Call load_context() first.")
        
        encoded_images = []
        
        for i, kf in enumerate(self.context_data['keyframes']):
            image_path = kf['image_path']
            if os.path.exists(image_path):
                # åŠ è½½å¹¶è°ƒæ•´å›¾åƒå¤§å°
                image = Image.open(image_path)
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
                
                # æ·»åŠ æ•°å­—æ ‡ç­¾
                if add_labels:
                    image = self._add_frame_label(image, i + 1)
                
                # è½¬æ¢ä¸ºbase64
                buffer = BytesIO()
                image.save(buffer, format='JPEG', quality=85)
                image_b64 = base64.b64encode(buffer.getvalue()).decode()
                encoded_images.append(image_b64)
            else:
                print(f"Warning: Image not found: {image_path}")
        
        return encoded_images
    
    def _add_frame_label(self, image: Image.Image, frame_number: int) -> Image.Image:
        """
        åœ¨å›¾ç‰‡å·¦ä¸Šè§’æ·»åŠ å¸§ç¼–å·æ ‡ç­¾
        
        Args:
            image: PILå›¾åƒ
            frame_number: å¸§ç¼–å·
            
        Returns:
            Image.Image: æ·»åŠ äº†æ ‡ç­¾çš„å›¾åƒ
        """
        from PIL import ImageDraw, ImageFont
        
        # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå›¾
        labeled_image = image.copy()
        draw = ImageDraw.Draw(labeled_image)
        
        # æ ‡ç­¾æ–‡æœ¬
        label_text = str(frame_number)
        
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å­—ä½“
        try:
            # Windowsç³»ç»Ÿå­—ä½“
            font_size = max(20, min(image.width, image.height) // 20)
            font = ImageFont.truetype("arial.ttf", font_size)
        except:
            try:
                # å¤‡ç”¨å­—ä½“
                font_size = max(20, min(image.width, image.height) // 20)
                font = ImageFont.load_default()
            except:
                font = None
        
        # è·å–æ–‡æœ¬å°ºå¯¸
        if font:
            bbox = draw.textbbox((0, 0), label_text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
        else:
            text_width, text_height = 20, 15
        
        # æ ‡ç­¾ä½ç½®å’Œå°ºå¯¸
        padding = 5
        label_width = text_width + 2 * padding
        label_height = text_height + 2 * padding
        
        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
        overlay = Image.new('RGBA', labeled_image.size, (0, 0, 0, 0))
        overlay_draw = ImageDraw.Draw(overlay)
        
        # èƒŒæ™¯çŸ©å½¢ (çº¢è‰²åŠé€æ˜)
        overlay_draw.rectangle(
            [(0, 0), (label_width, label_height)],
            fill=(255, 0, 0, 180)  # çº¢è‰²èƒŒæ™¯ï¼Œ180é€æ˜åº¦
        )
        
        # åˆå¹¶èƒŒæ™¯
        labeled_image = Image.alpha_composite(labeled_image.convert('RGBA'), overlay).convert('RGB')
        
        # é‡æ–°åˆ›å»ºç»˜åˆ¶å¯¹è±¡
        draw = ImageDraw.Draw(labeled_image)
        
        # ç»˜åˆ¶ç™½è‰²æ–‡å­—
        text_x = padding
        text_y = padding
        
        if font:
            draw.text((text_x, text_y), label_text, fill=(255, 255, 255), font=font)
        else:
            draw.text((text_x, text_y), label_text, fill=(255, 255, 255))
        
        return labeled_image
    
    def get_vlm_prompt(self) -> str:
        """è·å–VLMæç¤ºè¯"""
        if not self.context_data:
            raise ValueError("No context data loaded.")
        return self.context_data['vlm_prompt']
    
    def create_lmstudio_messages(self, custom_prompt: Optional[str] = None) -> List[Dict]:
        """
        åˆ›å»ºLMStudio APIæ¶ˆæ¯æ ¼å¼
        
        Args:
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            
        Returns:
            List[Dict]: LMStudioæ¶ˆæ¯æ ¼å¼
        """
        if not self.context_data:
            raise ValueError("No context data loaded.")
        
        # å‡†å¤‡å›¾åƒ
        images_b64 = self.prepare_images_for_vlm()
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = []
        
        # æ·»åŠ æ–‡æœ¬æç¤º
        prompt_text = custom_prompt or self.get_vlm_prompt()
        content.append({
            "type": "text",
            "text": prompt_text
        })
        
        # æ·»åŠ å›¾åƒ
        for i, image_b64 in enumerate(images_b64):
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{image_b64}"
                }
            })
        
        return [
            {
                "role": "system",
                "content": "You are an expert video analyst. Analyze the provided sequence of key frames and their context to understand the video content."
            },
            {
                "role": "user",
                "content": content
            }
        ]
    
    def analyze_with_lmstudio(self, custom_prompt: Optional[str] = None, model: str = "llava") -> Dict:
        """
        ä½¿ç”¨LMStudioåˆ†æè§†é¢‘
        
        Args:
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            model: æ¨¡å‹åç§°
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        messages = self.create_lmstudio_messages(custom_prompt)
        
        print(f"Sending {len(self.context_data['keyframes'])} frames to LMStudio...")
        
        # LMStudio APIè°ƒç”¨
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 2000,
            "temperature": 0.1,
            "stream": False
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    'analysis': result['choices'][0]['message']['content'],
                    'usage': result.get('usage', {}),
                    'model': model
                }
            else:
                raise Exception(f"LMStudio API error: {response.status_code} - {response.text}")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"Failed to connect to LMStudio: {e}")
    
    def export_for_manual_upload(self, output_dir: str = "lmstudio_export"):
        """å¯¼å‡ºæ–‡ä»¶ç”¨äºæ‰‹åŠ¨ä¸Šä¼ åˆ°LMStudio"""
        if not self.context_data:
            raise ValueError("No context data loaded.")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†å›¾ç‰‡å¹¶æ·»åŠ æ ‡ç­¾
        for i, kf in enumerate(self.context_data['keyframes']):
            if os.path.exists(kf['image_path']):
                # åŠ è½½åŸå›¾
                image = Image.open(kf['image_path'])
                
                # æ·»åŠ æ•°å­—æ ‡ç­¾
                labeled_image = self._add_frame_label(image, i + 1)
                
                # ä¿å­˜å¸¦æ ‡ç­¾çš„å›¾ç‰‡
                dest_path = os.path.join(output_dir, f"frame_{i+1:02d}_{kf['image_filename']}")
                labeled_image.save(dest_path, 'JPEG', quality=95)
        
        # å¯¼å‡ºæç¤ºè¯
        prompt_file = os.path.join(output_dir, "prompt.txt")
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write("=== LMStudio VLM Analysis Prompt ===\n\n")
            f.write(self.get_vlm_prompt())
        
        print(f"âœ… Files exported to: {output_dir}")
        print(f"ğŸ“„ Prompt: {prompt_file}")
        print(f"ğŸ–¼ï¸  Images: {len(self.context_data['keyframes'])} frames")
        
        return output_dir

def demo_analysis():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LMStudio VLMæ¥å£"""
    print("=== LMStudio VLM Interface Demo ===")
    
    # æ£€æŸ¥ä¸Šä¸‹æ–‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    context_file = "vlm_context_output/vlm_context.json"
    if not os.path.exists(context_file):
        print(f"Context file not found: {context_file}")
        print("Please run crl_vlm_context_builder.py first to generate the context.")
        return
    
    # LMStudioæ¥å£é€‰é¡¹
    print("\nLMStudio VLM Analysis Options:")
    print("1. Auto API call (requires LMStudio server running)")
    print("2. Export for manual upload")
    
    choice = input("Select option (1-2): ").strip()
    
    interface = LMStudioVLMInterface()
    interface.load_context(context_file)
    
    if choice == "1":
        # è‡ªåŠ¨APIè°ƒç”¨
        try:
            # æ£€æŸ¥LMStudioæ˜¯å¦è¿è¡Œ
            test_response = requests.get(f"{interface.base_url}/v1/models", timeout=5)
            if test_response.status_code != 200:
                raise Exception("LMStudio server not responding")
            
            print("âœ… LMStudio server detected")
            
            # è·å–å¯ç”¨æ¨¡å‹
            models = test_response.json()
            if models.get('data'):
                print(f"Available models: {[m['id'] for m in models['data']]}")
                model_name = models['data'][0]['id']  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
            else:
                model_name = "llava"  # é»˜è®¤æ¨¡å‹å
            
            result = interface.analyze_with_lmstudio(model=model_name)
            
            print("\n=== LMStudio Analysis ===")
            print(result['analysis'])
            
            # ä¿å­˜ç»“æœ
            output_file = "vlm_context_output/lmstudio_analysis.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("=== LMStudio VLM Analysis ===\n\n")
                f.write(result['analysis'])
                f.write(f"\n\n=== Model Info ===\n")
                f.write(f"Model: {result['model']}\n")
                f.write(f"Usage: {result['usage']}\n")
            
            print(f"\nğŸ“„ Analysis saved to: {output_file}")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("ğŸ’¡ Make sure LMStudio is running with a vision model loaded")
            print("ğŸ’¡ Default server: http://localhost:1234")
    
    elif choice == "2":
        # å¯¼å‡ºç”¨äºæ‰‹åŠ¨ä¸Šä¼ 
        export_dir = interface.export_for_manual_upload()
        print(f"\nâœ… Files ready for manual upload to LMStudio")
        print(f"ğŸ“ Location: {export_dir}")
        print("\nğŸ“‹ Next steps:")
        print("1. Open LMStudio and load a vision-language model")
        print("2. Start a chat session")
        print("3. Upload all frame images from the export folder")
        print("4. Copy and paste the prompt from prompt.txt")
        print("5. Submit for analysis")
    
    else:
        print("Invalid choice")

if __name__ == "__main__":
    demo_analysis()